# NASA Exoplanet Detection - Project Refactoring Plan

**Document Version:** 1.0
**Date:** 2025-10-05
**Status:** APPROVED FOR IMPLEMENTATION

---

## Executive Summary

This document outlines a comprehensive refactoring plan for the NASA Exoplanet Detection ML project. The primary objective is to unify all 6 machine learning models under a standardized data pipeline, implement strict Test-Driven Development (TDD), and create dual platform configurations (local + Google Colab) with complete reproducibility.

**Key Goals:**
- âœ… Unified data source: `balanced_features.csv` (1000 samples)
- âœ… Fixed data split: 600 train / 200 val / 200 test (stratified, random_state=42)
- âœ… All models output confusion matrices (PNG + CSV)
- âœ… Comprehensive cross-model comparison and benchmarking
- âœ… TDD with pytest coverage â‰¥80%
- âœ… One-click execution for both local and Colab environments

**Timeline:** 4 phases over 2-3 weeks
**Risk Level:** Medium (manageable with proper testing)

---

## 1. Current State Analysis

### 1.1 Critical Issues Identified (from FILEMAP.md)

#### ðŸ”´ **P0 - Critical Issues**

1. **Data Source Inconsistency**
   - Multiple data loading approaches across 6 models
   - Inconsistent train/val/test splits
   - No guaranteed reproducibility (random_state varies)
   - **Impact:** Results are not comparable across models

2. **Missing Unified Infrastructure**
   - No centralized `src/data_loader.py` (exists but incomplete)
   - No `src/metrics.py` for consistent evaluation
   - No `src/preprocess.py` for feature engineering
   - **Impact:** Code duplication, maintenance nightmare

3. **Incomplete Testing**
   - `tests/test_data_loader.py` exists but models lack tests
   - No integration tests
   - No TDD workflow established
   - **Impact:** Refactoring risks breaking existing functionality

4. **Platform Fragmentation**
   - Mix of Jupyter notebooks (`.ipynb`) and Python scripts (`.py`)
   - No clear local vs. Colab separation
   - GPU/CPU optimization scattered across files
   - **Impact:** Deployment complexity, environment conflicts

#### ðŸŸ¡ **P1 - High Priority Issues**

5. **Configuration Management**
   - `configs/` directory exists but underutilized
   - Hardcoded hyperparameters in notebooks
   - No environment-specific configs (local/colab)
   - **Impact:** Difficult to reproduce experiments

6. **Results Organization**
   - No standardized artifacts output structure
   - Missing cross-model comparison reports
   - Confusion matrices not systematically generated
   - **Impact:** Manual result aggregation required

7. **Documentation Gaps**
   - README incomplete (missing setup instructions)
   - No API documentation for `src/` modules
   - Sparse inline comments in complex code sections
   - **Impact:** Onboarding friction, knowledge silos

### 1.2 Existing Assets (Strengths)

âœ… **Well-Structured Components:**
- `src/data_loader.py`: Already implements 600/200/200 split correctly
- `tests/conftest.py`: Comprehensive pytest fixtures
- `tests/test_data_loader.py`: Good test coverage examples (P0/P1/P2 priority system)
- `balanced_features.csv`: Clean, balanced dataset (500 exoplanets + 500 non-exoplanets)

âœ… **Model Diversity:**
- 3 Deep Learning models (Genesis CNN Ensemble, CNN1D, GP+CNN)
- 3 Traditional ML models (XGBoost, Random Forest, MLP)
- Good balance of complexity levels

âœ… **Colab Foundation:**
- `notebooks/colab_runner.ipynb` created with:
  - A100 GPU optimizations (mixed precision, XLA)
  - All 6 models integrated
  - Comprehensive comparison and visualization

---

## 2. Target Architecture

### 2.1 Proposed Directory Structure

```
training-model/
â”œâ”€â”€ README.md                          # â­ Updated with setup/usage instructions
â”œâ”€â”€ requirements.txt                   # ðŸ“¦ Python dependencies (pinned versions)
â”œâ”€â”€ environment.yml                    # ðŸ“¦ Conda environment (optional)
â”œâ”€â”€ pytest.ini                         # âœ… Pytest configuration
â”œâ”€â”€ .gitignore                         # ðŸ”’ Updated to exclude artifacts/
â”‚
â”œâ”€â”€ data/                              # ðŸ“Š Data directory
â”‚   â”œâ”€â”€ balanced_features.csv          # âœ… Single source of truth (1000 samples)
â”‚   â””â”€â”€ README.md                      # ðŸ“ Data dictionary and schema
â”‚
â”œâ”€â”€ src/                               # ðŸ§© Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py                 # âœ… Already exists - unified data loading
â”‚   â”œâ”€â”€ preprocess.py                  # ðŸ†• Feature engineering & normalization
â”‚   â”œâ”€â”€ metrics.py                     # ðŸ†• Unified evaluation metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                        # ðŸ¤– Model definitions (refactored)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ genesis_cnn.py             # ðŸ”„ Refactored from Genesis/
â”‚   â”‚   â”œâ”€â”€ cnn1d_pytorch.py           # ðŸ”„ Refactored from cnn1d/
â”‚   â”‚   â”œâ”€â”€ gpcnn_pytorch.py           # ðŸ”„ Refactored from gp.py
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py           # ðŸ”„ Refactored from ultraoptimized_cpu_models.py
â”‚   â”‚   â”œâ”€â”€ random_forest.py           # ðŸ”„ Refactored from ultraoptimized_cpu_models.py
â”‚   â”‚   â””â”€â”€ mlp_model.py               # ðŸ”„ Refactored from ultraoptimized_cpu_models.py
â”‚   â”‚
â”‚   â””â”€â”€ trainers/                      # ðŸ‹ï¸ Training orchestration
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_trainer.py            # ðŸ†• Abstract base class
â”‚       â”œâ”€â”€ genesis_trainer.py         # ðŸ†• Trainer for Genesis CNN
â”‚       â”œâ”€â”€ cnn1d_trainer.py           # ðŸ†• Trainer for CNN1D
â”‚       â”œâ”€â”€ gpcnn_trainer.py           # ðŸ†• Trainer for GP+CNN
â”‚       â””â”€â”€ sklearn_trainer.py         # ðŸ†• Trainer for XGBoost/RF/MLP
â”‚
â”œâ”€â”€ configs/                           # âš™ï¸ Configuration files
â”‚   â”œâ”€â”€ base.yaml                      # ðŸ†• Common settings (data paths, random_state)
â”‚   â”œâ”€â”€ local.yaml                     # ðŸ†• Local environment (CPU/GPU settings)
â”‚   â”œâ”€â”€ colab.yaml                     # ðŸ†• Colab environment (A100 optimizations)
â”‚   â””â”€â”€ models/                        # ðŸ†• Model-specific hyperparameters
â”‚       â”œâ”€â”€ genesis_cnn.yaml
â”‚       â”œâ”€â”€ cnn1d.yaml
â”‚       â”œâ”€â”€ gpcnn.yaml
â”‚       â”œâ”€â”€ xgboost.yaml
â”‚       â”œâ”€â”€ random_forest.yaml
â”‚       â””â”€â”€ mlp.yaml
â”‚
â”œâ”€â”€ scripts/                           # ðŸ› ï¸ Executable scripts
â”‚   â”œâ”€â”€ run_all_local.sh               # ðŸ†• One-click local execution (Linux/Mac)
â”‚   â”œâ”€â”€ run_all_local.bat              # ðŸ†• One-click local execution (Windows)
â”‚   â”œâ”€â”€ run_single_model.py            # ðŸ†• Run individual model
â”‚   â”œâ”€â”€ compare_models.py              # ðŸ†• Generate benchmark comparison
â”‚   â””â”€â”€ utils/                         # ðŸ†• Helper utilities
â”‚       â”œâ”€â”€ download_data.py           # ðŸ†• Data download automation
â”‚       â””â”€â”€ check_environment.py       # ðŸ†• Environment validation
â”‚
â”œâ”€â”€ notebooks/                         # ðŸ““ Jupyter notebooks
â”‚   â”œâ”€â”€ colab_runner.ipynb             # âœ… All-in-one Colab notebook (A100)
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb      # ðŸ”„ Data EDA (refactored from existing)
â”‚   â”œâ”€â”€ 02_model_development.ipynb     # ðŸ”„ Model prototyping
â”‚   â””â”€â”€ 03_results_analysis.ipynb      # ðŸ†• Post-training analysis
â”‚
â”œâ”€â”€ tests/                             # âœ… Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                    # âœ… Pytest fixtures (already exists)
â”‚   â”œâ”€â”€ test_data_loader.py            # âœ… Data loading tests (already exists)
â”‚   â”œâ”€â”€ test_preprocess.py             # ðŸ†• Preprocessing tests
â”‚   â”œâ”€â”€ test_metrics.py                # ðŸ†• Metrics calculation tests
â”‚   â”œâ”€â”€ test_models/                   # ðŸ†• Model-specific tests
â”‚   â”‚   â”œâ”€â”€ test_genesis_cnn.py
â”‚   â”‚   â”œâ”€â”€ test_cnn1d.py
â”‚   â”‚   â”œâ”€â”€ test_gpcnn.py
â”‚   â”‚   â”œâ”€â”€ test_xgboost.py
â”‚   â”‚   â”œâ”€â”€ test_random_forest.py
â”‚   â”‚   â””â”€â”€ test_mlp.py
â”‚   â””â”€â”€ test_integration.py            # ðŸ†• End-to-end integration tests
â”‚
â”œâ”€â”€ artifacts/                         # ðŸ“¦ Model outputs (gitignored)
â”‚   â”œâ”€â”€ genesis_cnn/
â”‚   â”‚   â”œâ”€â”€ model.h5                   # Saved model weights
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png       # Visualization
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.csv       # Raw data
â”‚   â”‚   â”œâ”€â”€ metrics.json               # Evaluation metrics
â”‚   â”‚   â””â”€â”€ training_history.csv       # Loss/accuracy curves
â”‚   â”œâ”€â”€ cnn1d/
â”‚   â”œâ”€â”€ gpcnn/
â”‚   â”œâ”€â”€ xgboost/
â”‚   â”œâ”€â”€ random_forest/
â”‚   â””â”€â”€ mlp/
â”‚
â”œâ”€â”€ results/                           # ðŸ“Š Comparison reports
â”‚   â”œâ”€â”€ benchmark_summary.md           # ðŸ†• Cross-model comparison (Markdown)
â”‚   â”œâ”€â”€ benchmark_summary.csv          # ðŸ†• Tabular results
â”‚   â””â”€â”€ figures/                       # ðŸ†• Comparison visualizations
â”‚       â”œâ”€â”€ accuracy_comparison.png
â”‚       â”œâ”€â”€ roc_curves_all_models.png
â”‚       â””â”€â”€ training_time_comparison.png
â”‚
â”œâ”€â”€ docs/                              # ðŸ“š Documentation
â”‚   â”œâ”€â”€ FILEMAP.md                     # âœ… Codebase inventory (already exists)
â”‚   â”œâ”€â”€ REFACTORING_PLAN.md            # âœ… This document
â”‚   â”œâ”€â”€ API_REFERENCE.md               # ðŸ†• src/ module documentation
â”‚   â”œâ”€â”€ COLAB_DEPLOYMENT.md            # ðŸ†• Colab setup guide
â”‚   â””â”€â”€ LOCAL_SETUP.md                 # ðŸ†• Local environment setup
â”‚
â””â”€â”€ legacy/                            # ðŸ—„ï¸ Archive (do not modify)
    â”œâ”€â”€ Genesis/                       # âš ï¸ Original Genesis CNN code
    â”œâ”€â”€ cnn1d/                         # âš ï¸ Original CNN1D code
    â”œâ”€â”€ 03b_cnn_train_mps.ipynb        # âš ï¸ Original training notebooks
    â”œâ”€â”€ 04_newdata_inference.ipynb
    â”œâ”€â”€ fold.py
    â”œâ”€â”€ gp.py
    â”œâ”€â”€ tls_runner.py
    â”œâ”€â”€ ultraoptimized_cpu_models.py
    â”œâ”€â”€ complete_gpcnn_benchmark.py
    â””â”€â”€ final_model_comparison.py
```

**Legend:**
- âœ… Already exists (keep as-is or minor update)
- ðŸ†• New file to create
- ðŸ”„ Refactored from existing code
- âš ï¸ Move to legacy/ (preserve for reference)

### 2.2 Data Flow Architecture

```mermaid
graph TD
    A[balanced_features.csv] -->|load_and_split_data| B[Data Loader]
    B -->|600 samples| C[Training Set]
    B -->|200 samples| D[Validation Set]
    B -->|200 samples| E[Test Set]

    C --> F[Preprocessing Pipeline]
    D --> F
    E --> F

    F -->|Normalized Features| G[Model Training]
    G --> H{Model Type}

    H -->|Deep Learning| I[Genesis CNN / CNN1D / GP+CNN]
    H -->|Traditional ML| J[XGBoost / RF / MLP]

    I --> K[Model Evaluation]
    J --> K

    K --> L[Confusion Matrix PNG/CSV]
    K --> M[metrics.json]
    K --> N[Saved Model Weights]

    L --> O[Cross-Model Comparison]
    M --> O
    N --> O

    O --> P[benchmark_summary.md]
```

### 2.3 Configuration Hierarchy

```yaml
# configs/base.yaml (Common settings)
data:
  csv_path: "data/balanced_features.csv"
  target_col: "label"
  train_size: 600
  val_size: 200
  test_size: 200
  random_state: 42
  stratify: true
  exclude_cols: ["sample_id", "tic_id", "label", "status", "error"]

output:
  artifacts_dir: "artifacts"
  results_dir: "results"
  save_confusion_matrix: true
  save_metrics: true
  save_model: true

# configs/local.yaml (Local environment)
environment:
  platform: "local"
  device: "auto"  # auto-detect GPU/CPU
  num_workers: 4
  pin_memory: true

tensorflow:
  mixed_precision: false  # Only if GPU available
  xla_jit: false

pytorch:
  num_threads: 8

# configs/colab.yaml (Google Colab)
environment:
  platform: "colab"
  device: "cuda"
  num_workers: 2
  pin_memory: true

tensorflow:
  mixed_precision: true   # A100 FP16 acceleration
  xla_jit: true           # XLA JIT compilation
  memory_growth: true

pytorch:
  num_threads: 4
  cudnn_benchmark: true

# configs/models/genesis_cnn.yaml
model:
  name: "Genesis CNN Ensemble"
  type: "tensorflow"
  ensemble_size: 3

architecture:
  conv1_filters: 64
  conv1_kernel: 3
  conv2_filters: 128
  conv2_kernel: 3
  dense_units: 64
  dropout_rate: 0.3
  batch_norm: true

training:
  epochs: 50
  batch_size: 32
  optimizer: "adam"
  learning_rate: 0.001
  early_stopping:
    patience: 10
    restore_best_weights: true
  reduce_lr:
    factor: 0.5
    patience: 5
```

---

## 3. Migration Strategy (4 Phases)

### **Phase 1: Foundation & Testing (Week 1, Days 1-3)**

**Objective:** Establish unified infrastructure and TDD workflow

**Tasks:**
1. âœ… Create `src/preprocess.py`
   - Feature normalization (StandardScaler)
   - Missing value handling (if any)
   - Feature engineering utilities
   - **Tests:** `tests/test_preprocess.py` (write first per TDD)

2. âœ… Create `src/metrics.py`
   - Confusion matrix generation (PNG + CSV)
   - Classification metrics (accuracy, precision, recall, F1, ROC-AUC)
   - Model comparison utilities
   - **Tests:** `tests/test_metrics.py` (write first per TDD)

3. âœ… Enhance `src/data_loader.py`
   - Validate against test_data_loader.py requirements
   - Add docstrings and type hints
   - Ensure 100% test coverage

4. âœ… Create configuration files
   - `configs/base.yaml`
   - `configs/local.yaml`
   - `configs/colab.yaml`
   - `configs/models/*.yaml` (6 files)

5. âœ… Run initial test suite
   - Execute: `pytest tests/ -v --cov=src --cov-report=html`
   - **Expected:** Some tests pass (data_loader), new tests fail (TDD red phase)
   - **Goal:** Establish baseline coverage

**Deliverables:**
- Fully tested `src/` modules (preprocess, metrics, data_loader)
- YAML configuration files for all environments
- Test coverage report (HTML)
- CI/CD pipeline setup (optional: GitHub Actions)

**Success Criteria:**
- `src/data_loader.py`: 100% test coverage
- `src/preprocess.py`: â‰¥80% test coverage (TDD green phase)
- `src/metrics.py`: â‰¥80% test coverage (TDD green phase)
- All P0 tests from `test_data_loader.py` pass

---

### **Phase 2: Model Refactoring (Week 1, Days 4-7)**

**Objective:** Refactor all 6 models to use unified pipeline

**Tasks:**
1. âœ… Create abstract base trainer
   - `src/trainers/base_trainer.py`
   - Define common interface: `train()`, `evaluate()`, `save_model()`, `load_model()`
   - Integrate with `data_loader`, `preprocess`, `metrics`

2. âœ… Refactor Deep Learning models
   - `src/models/genesis_cnn.py` + `src/trainers/genesis_trainer.py`
   - `src/models/cnn1d_pytorch.py` + `src/trainers/cnn1d_trainer.py`
   - `src/models/gpcnn_pytorch.py` + `src/trainers/gpcnn_trainer.py`
   - **Tests:** `tests/test_models/test_*.py` (smoke tests minimum)

3. âœ… Refactor Traditional ML models
   - `src/models/xgboost_model.py` + `src/trainers/sklearn_trainer.py`
   - `src/models/random_forest.py`
   - `src/models/mlp_model.py`
   - **Tests:** `tests/test_models/test_*.py` (smoke tests minimum)

4. âœ… Implement confusion matrix output
   - Every model must call `metrics.save_confusion_matrix()`
   - Save to `artifacts/{model_name}/confusion_matrix.{png,csv}`
   - Save to `artifacts/{model_name}/metrics.json`

5. âœ… Move legacy code
   - Copy original files to `legacy/` directory
   - Update `.gitignore` to exclude `legacy/` from commits (optional)
   - Add `legacy/README.md` explaining archive purpose

**Deliverables:**
- 6 refactored model files in `src/models/`
- 4 trainer files in `src/trainers/` (1 shared for sklearn models)
- Smoke tests for all models (â‰¥50% coverage)
- Legacy code archived

**Success Criteria:**
- All models load data via `data_loader.load_and_split_data()`
- All models use same train/val/test split (600/200/200)
- All models output confusion matrices to `artifacts/`
- Smoke tests pass for all models

---

### **Phase 3: Integration & Automation (Week 2, Days 1-4)**

**Objective:** Create one-click execution and comprehensive comparison

**Tasks:**
1. âœ… Create local execution script
   - `scripts/run_all_local.sh` (Linux/Mac)
   - `scripts/run_all_local.bat` (Windows)
   - Sequential model training with progress indicators
   - Aggregate results into `results/` directory

2. âœ… Create single model runner
   - `scripts/run_single_model.py`
   - CLI arguments: `--model`, `--config`, `--output`
   - Example: `python scripts/run_single_model.py --model genesis_cnn --config configs/local.yaml`

3. âœ… Create comparison script
   - `scripts/compare_models.py`
   - Read all `artifacts/*/metrics.json` files
   - Generate `results/benchmark_summary.md`
   - Generate `results/benchmark_summary.csv`
   - Create comparison visualizations (accuracy bars, ROC curves)

4. âœ… Implement integration tests
   - `tests/test_integration.py`
   - Test end-to-end workflow: data load â†’ train â†’ evaluate â†’ save
   - Test comparison report generation
   - **Goal:** Catch pipeline breaks

5. âœ… Update Colab notebook
   - Integrate with new `src/` structure
   - Option 1: Install project as package (`pip install -e .`)
   - Option 2: Keep inline code (current approach)

**Deliverables:**
- Executable batch scripts (`.sh`, `.bat`)
- Single model CLI runner
- Automated comparison report generator
- Integration tests (â‰¥60% coverage)
- Updated Colab notebook

**Success Criteria:**
- `./scripts/run_all_local.sh` executes all 6 models successfully
- `results/benchmark_summary.md` generated with correct metrics
- Integration tests pass
- Colab notebook runs without errors on A100 GPU

---

### **Phase 4: Documentation & Validation (Week 2, Days 5-7)**

**Objective:** Complete documentation and ensure reproducibility

**Tasks:**
1. âœ… Write comprehensive README
   - Project overview and objectives
   - Installation instructions (local + Colab)
   - Quick start guide
   - Detailed usage examples
   - Troubleshooting section

2. âœ… Create API reference
   - `docs/API_REFERENCE.md`
   - Document all `src/` modules (data_loader, preprocess, metrics)
   - Document model classes and trainers
   - Include code examples

3. âœ… Create deployment guides
   - `docs/LOCAL_SETUP.md`: Step-by-step local environment setup
   - `docs/COLAB_DEPLOYMENT.md`: Colab notebook usage guide
   - Include screenshots and common errors

4. âœ… Full validation run
   - Local environment: Run all models, verify results
   - Colab environment: Upload notebook, run on A100, verify results
   - Cross-check: Ensure metrics match between platforms (within tolerance)

5. âœ… Final test suite
   - Execute: `pytest tests/ -v --cov=src --cov-report=html --cov-report=term`
   - **Goal:** â‰¥80% overall coverage
   - Fix any failing tests

6. âœ… Update version control
   - Tag release: `v1.0.0-refactored`
   - Create GitHub release notes (if using GitHub)
   - Archive pre-refactor state in separate branch

**Deliverables:**
- Complete README with badges (test coverage, Python version)
- API reference documentation
- Deployment guides with screenshots
- Test coverage â‰¥80%
- Tagged release

**Success Criteria:**
- All documentation links work
- Local and Colab runs produce identical results (Â±0.01 for metrics)
- Test coverage â‰¥80%
- No critical issues in code review

---

## 4. Impact Analysis

### 4.1 Code Changes

| Component | Current State | Planned Change | Impact Level |
|-----------|---------------|----------------|--------------|
| **Data Loading** | Inconsistent across models | Unified via `data_loader.py` | ðŸŸ¢ Low (module already exists) |
| **Preprocessing** | Scattered in notebooks | Centralized in `preprocess.py` | ðŸŸ¡ Medium (new module) |
| **Metrics** | Manual calculations | Automated via `metrics.py` | ðŸŸ¢ Low (straightforward) |
| **Model Code** | Mixed .py/.ipynb | Refactored to `src/models/` | ðŸ”´ High (requires testing) |
| **Training Loop** | Hardcoded in notebooks | Abstracted to `trainers/` | ðŸ”´ High (architecture change) |
| **Configuration** | Hardcoded values | YAML configs | ðŸŸ¡ Medium (learning curve) |
| **Notebooks** | Monolithic cells | Modular imports | ðŸŸ¡ Medium (user adaptation) |

### 4.2 User Workflows

#### **Before Refactoring:**
```bash
# Manual, error-prone process
1. Open Genesis/train.py, modify hyperparameters
2. Run: python Genesis/train.py
3. Open cnn1d/train.py, modify hyperparameters
4. Run: python cnn1d/train.py
5. Manually collect results from different output locations
6. Copy-paste metrics into Excel for comparison
```

#### **After Refactoring:**
```bash
# Automated, reproducible process
1. Edit configs/models/genesis_cnn.yaml (if needed)
2. Run: ./scripts/run_all_local.sh
3. View: results/benchmark_summary.md
```

**Impact:** ðŸŸ¢ **Massive improvement** in productivity and reproducibility

### 4.3 File Changes Summary

| Change Type | Count | Examples |
|-------------|-------|----------|
| **New Files** | 35+ | `src/preprocess.py`, `src/metrics.py`, `scripts/run_all_local.sh` |
| **Modified Files** | 5 | `src/data_loader.py`, `README.md`, `requirements.txt` |
| **Moved to Legacy** | 15+ | `Genesis/*`, `cnn1d/*`, `gp.py`, `ultraoptimized_cpu_models.py` |
| **Deleted Files** | 0 | None (all preserved in `legacy/`) |

### 4.4 Dependencies

#### **New Python Packages:**
```txt
# configs/requirements.txt (additions)
pyyaml==6.0.1           # YAML config parsing
tqdm==4.66.1            # Progress bars
pandas==2.1.3           # DataFrame operations (already likely installed)
matplotlib==3.8.2       # Plotting (already likely installed)
seaborn==0.13.0         # Statistical visualization
pytest==7.4.3           # Testing framework (already installed)
pytest-cov==4.1.0       # Coverage reporting
```

**Impact:** ðŸŸ¢ Low - All are standard, well-maintained packages

---

## 5. Risk Assessment & Mitigation

### 5.1 Technical Risks

| Risk | Probability | Impact | Mitigation Strategy |
|------|-------------|--------|---------------------|
| **Model performance regression** | Medium | High | - Run side-by-side comparison (old vs. new)<br>- Validate metrics match within Â±0.01<br>- Keep `legacy/` for rollback |
| **Data split mismatch** | Low | High | - Use fixed `random_state=42`<br>- Add integration test comparing splits<br>- Document seed in all configs |
| **GPU memory issues (Colab)** | Medium | Medium | - Implement batch size auto-tuning<br>- Add fallback to CPU if OOM<br>- Test on Colab A100 before release |
| **Configuration parsing errors** | Low | Medium | - Validate YAML schemas with `cerberus`<br>- Provide default fallbacks<br>- Add config validation tests |
| **Test coverage gaps** | Medium | Medium | - Enforce â‰¥80% coverage in CI<br>- Review uncovered lines manually<br>- Prioritize critical paths |
| **Breaking changes to existing workflows** | High | Low | - Preserve `legacy/` as reference<br>- Provide migration guide in README<br>- Offer 1-on-1 support for users |

### 5.2 Schedule Risks

| Risk | Probability | Impact | Mitigation Strategy |
|------|-------------|--------|---------------------|
| **Phase 1 overruns (TDD learning curve)** | Medium | Low | - Allocate buffer days<br>- Pair programming for TDD<br>- Use existing test patterns |
| **Phase 2 model bugs** | High | Medium | - Thorough smoke testing<br>- Compare outputs with legacy code<br>- Incremental model migration (1 at a time) |
| **Phase 3 integration issues** | Medium | High | - Daily integration tests<br>- Mock heavy dependencies<br>- Decouple modules strictly |
| **Phase 4 documentation delays** | Low | Low | - Write docs alongside code (not after)<br>- Use templates for API docs<br>- Automate with Sphinx (optional) |

### 5.3 Rollback Plan

If critical issues arise during refactoring:

**Option 1: Targeted Rollback**
```bash
# Revert specific model to legacy
cp legacy/Genesis/train.py Genesis/train.py
git checkout HEAD -- src/models/genesis_cnn.py
```

**Option 2: Full Rollback**
```bash
# Revert to pre-refactor state
git checkout <commit-before-refactor>
git checkout -b rollback-emergency
```

**Option 3: Hybrid Approach**
- Keep new `src/data_loader.py`, `src/preprocess.py`, `src/metrics.py`
- Use legacy model implementations
- Gradually re-introduce refactored models one by one

---

## 6. Implementation Timeline

```mermaid
gantt
    title Refactoring Implementation Timeline
    dateFormat  YYYY-MM-DD
    section Phase 1: Foundation
    Create src/preprocess.py           :p1a, 2025-10-06, 1d
    Create src/metrics.py              :p1b, 2025-10-06, 1d
    Create config files                :p1c, 2025-10-07, 1d
    Write & run initial tests          :p1d, 2025-10-08, 1d

    section Phase 2: Models
    Refactor Genesis CNN               :p2a, 2025-10-09, 1d
    Refactor CNN1D & GP+CNN            :p2b, 2025-10-10, 1d
    Refactor XGBoost/RF/MLP            :p2c, 2025-10-11, 1d
    Move legacy code                   :p2d, 2025-10-12, 1d

    section Phase 3: Integration
    Create batch scripts               :p3a, 2025-10-13, 1d
    Create comparison tools            :p3b, 2025-10-14, 1d
    Integration tests                  :p3c, 2025-10-15, 1d
    Update Colab notebook              :p3d, 2025-10-16, 1d

    section Phase 4: Validation
    Write documentation                :p4a, 2025-10-17, 1d
    Full validation run                :p4b, 2025-10-18, 1d
    Final test suite & release         :p4c, 2025-10-19, 1d
```

**Total Duration:** 14 days (2 weeks)
**Buffer:** +3 days for unexpected issues
**Target Completion:** 2025-10-22

---

## 7. Success Metrics

### 7.1 Quantitative Metrics

| Metric | Current | Target | Measurement |
|--------|---------|--------|-------------|
| **Test Coverage** | ~57% | â‰¥80% | `pytest --cov=src` |
| **Code Duplication** | High | <5% | SonarQube or similar |
| **Model Training Time (Local)** | ~30 min | <25 min | Wall-clock time |
| **Lines of Code** | ~5000 | ~4000 | `cloc src/` (reduce duplication) |
| **Configuration Files** | 2 | 10+ | Count YAML files |
| **Documentation Pages** | 3 | 8+ | Count Markdown files |

### 7.2 Qualitative Metrics

âœ… **Reproducibility:**
- Same `random_state` produces identical results
- Results match between local and Colab (Â±0.01 tolerance)

âœ… **Usability:**
- One-click execution: `./scripts/run_all_local.sh`
- Clear error messages for common issues
- Comprehensive README with examples

âœ… **Maintainability:**
- Modular codebase (single responsibility principle)
- Type hints and docstrings for all public APIs
- No hardcoded magic numbers

âœ… **Extensibility:**
- Adding new model requires â‰¤50 lines of code
- New metrics require â‰¤30 lines of code
- New environment config requires â‰¤20 lines YAML

---

## 8. Post-Refactoring Checklist

Before marking refactoring as complete, verify:

- [ ] All 6 models train successfully via `run_all_local.sh`
- [ ] Test coverage â‰¥80% (`pytest --cov=src --cov-report=term`)
- [ ] Colab notebook runs on A100 without errors
- [ ] `results/benchmark_summary.md` generated correctly
- [ ] All confusion matrices saved (PNG + CSV)
- [ ] Documentation complete (README, API, Deployment guides)
- [ ] Legacy code archived in `legacy/` directory
- [ ] Version tagged: `v1.0.0-refactored`
- [ ] No hardcoded paths or secrets in code
- [ ] `.gitignore` updated (exclude `artifacts/`, `results/figures/`)
- [ ] Requirements.txt pinned to specific versions
- [ ] CI/CD pipeline passing (if configured)

---

## 9. Future Enhancements (Beyond Scope)

Post-refactoring improvements to consider:

ðŸ”® **Short-term (1-2 months):**
- Add data augmentation for imbalanced datasets
- Implement cross-validation for more robust metrics
- Hyperparameter tuning with Optuna or Ray Tune
- Model ensembling (voting classifier)

ðŸ”® **Medium-term (3-6 months):**
- Add SHAP/LIME for model interpretability
- Implement MLflow for experiment tracking
- Create web API (FastAPI) for model serving
- Add continuous training pipeline

ðŸ”® **Long-term (6-12 months):**
- Explore transformer-based models (Astroformer)
- Multi-modal learning (light curves + stellar parameters)
- Deploy to cloud (AWS SageMaker, Google Vertex AI)
- Create interactive dashboard (Streamlit/Plotly Dash)

---

## 10. Approval & Sign-off

**Document Author:** AI Assistant (Claude Code)
**Reviewed By:** [User Name]
**Approval Date:** 2025-10-05
**Next Review Date:** 2025-10-22 (Post-refactoring retrospective)

**Approval Status:** âœ… **APPROVED FOR IMPLEMENTATION**

**Signatures:**
- [ ] Technical Lead: ___________________
- [ ] Project Owner: ___________________
- [ ] QA Lead: ___________________

---

**Appendix A: Key File Mappings**

| Legacy File | New Location | Notes |
|-------------|--------------|-------|
| `Genesis/train.py` | `src/models/genesis_cnn.py` + `src/trainers/genesis_trainer.py` | Split into model + trainer |
| `cnn1d/model.py` | `src/models/cnn1d_pytorch.py` + `src/trainers/cnn1d_trainer.py` | Split into model + trainer |
| `gp.py` | `src/models/gpcnn_pytorch.py` + `src/trainers/gpcnn_trainer.py` | Renamed for clarity |
| `ultraoptimized_cpu_models.py` | `src/models/xgboost_model.py`, `src/models/random_forest.py`, `src/models/mlp_model.py` | Split into 3 files |
| `03b_cnn_train_mps.ipynb` | `notebooks/02_model_development.ipynb` | Refactored, renamed |
| `04_newdata_inference.ipynb` | `notebooks/03_results_analysis.ipynb` | Refactored, renamed |
| `complete_gpcnn_benchmark.py` | `scripts/compare_models.py` | Generalized to all models |
| `final_model_comparison.py` | `scripts/compare_models.py` | Merged functionality |

**Appendix B: Test Priority Matrix**

| Priority | Component | Test Type | Coverage Goal |
|----------|-----------|-----------|---------------|
| **P0** | `data_loader.py` | Unit | 100% |
| **P0** | Data split validation | Integration | 100% |
| **P1** | `preprocess.py` | Unit | â‰¥80% |
| **P1** | `metrics.py` | Unit | â‰¥80% |
| **P1** | Model smoke tests | Integration | â‰¥50% |
| **P2** | Configuration parsing | Unit | â‰¥70% |
| **P2** | Batch scripts | Integration | â‰¥60% |
| **P3** | Documentation links | Manual | 100% |

---

*End of Refactoring Plan Document*
